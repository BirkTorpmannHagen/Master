% !TeX spellcheck = en_US
\chapter{Background} \label{background}
\setcounter{chapter}{2}
\section{Colorectal Polyps, Medical Imaging, and Deep Learning}
	Polyps are small growths found in and around the inner lining of the large intestine. These polyps, also referred to as adenomas, can in time develop into cancerous tumours, or carcinomas, in a process known as the adenoma-carcinoma sequence \cite{ACS}. Though the majority of polyps do not undergo this process, identifying polyps nonetheless constitutes an important step towards preventing colorectal cancer. Indeed, resection of these polyps has been shown to reduce the incidence of colorectal cancer by a significant margin \cite{resection}. 
	
	Though colorectal cancer remains as one of the leading causes of cancer-related death worldwide (source), mortality rates have nonetheless declined in large part to increased use of screening colonoscopy, which in turn has facilitated the use of more preemptive treatment. Polyps are, however, by nature somewhat difficult to detect and are routinely missed by clinicians, with miss rates ranging upwards of 27\% for diminutive (<2.5mm) polyps  \cite{missrate1, missrate2}.
	
	Reducing this miss rate has the potential to further reduce incidence rates. As a result, there has been a significant body of work dedicated to developing systems and techniques to aid in optimizing and effectivizing the screening procedure. One such example, referred to as chromoendoscopy, has been shown to reduce miss rates by (...) merely by employing the use of specific dyes prior to the colonoscopy. Similarly, the use of narrow-band imaging techniques, wherein light of specific wavelengths specifically designed to highlight the textural differences between the polyps and the surrounding tissue, has been shown to reduce miss rates by (...) 
	
	These systems do, however, require more equipment, training and expertise to effectively employ. Thus, automatic polyp segmentation using deep learning and convolutional neural networks (CNNs) has been identified as a possible diminutive detection method. This requires minimal training time on the part of the clinician, no additional equipment, and has been show to significantly increase detection rates when deployed in a clinical setting \cite{polyp-success-story}. 

	This has spurred on a large body of research dedicated to improving on the performance and expanding the capabilities of deep-learning based systems for polyp detection and segmentation. Several challenges been also held, namely the Endotect challenge \cite{endotect}, EndoCV2020 \cite{endocv2020}, EndoCV2021 \cite{endocv2021}, and more.
	
	There are, however, still several hurdles to overcome; recent research has shown that even state of the art deep-learning pipelines are prone to generalization failure when deployed in practical settings, particularly when exposed to distributional shifts such as changes in demographics, imaging equipment, noise, and more despite exhibiting high performance on hold-out sets \cite{retinopathy, damour2020underspecification, pneumonia, shortcut_learning}. As a result, the EndoCV2021 challange emloyed training data from several centers, with the data from one of the centers being hidden and used as generalization test data. The results from this challange demomstrated the pervasiveness of generalization failure, with every submitted model exhibiting significant performance reductions when evaluated on their hidden dataset (cite summary here). 
	
	Naturally, automatic segmentation systems are rendered practically useless should they fail to perform sufficiently outside of the very carefully controlled conditions upon which they are trained. Consequently, for any such system to have any practical merit, it has to have the capacity to infer causally reasonable patterns in the data that generalize well to other hospitals, demographics, imaging equipment, resolutions, and so on. Though a human would not find this type of generalization very 

\section{Generalization failure in broader contexts} \label{case_studies}
	\subsection{Generalization failure in Medical Imaging}
	Generalization failure is not, of course, unique to the gastrointestinal domain. Indeed, though medical imaging has in recent years proven to be one of the most promising applications of artificial intelligence and deep learning, having the capacity to significantly improve both the accuracy and efficiency of detection, diagnosis, and treatment of a wide variety of diseases \cite{dl_medical_imaging}, they are nonetheless highly prone to generalization failure. In addition to the already limited capabilities of deep neural networks to generalise, medical domains are subject to a number of other exacerbating factors that make generalization all the more diffcult. Training data is often scarce, the pathologies that constitute the classification targets are unevenly distributed and often exhibit high degrees of inter-class and within-class variability. Moreover, due to the sheer scope of the data involved, there are inevitably a significant number of confounding variables both during training and in deployment.  
		
	For instance, a deep-learning based classifier which successfully detected pneumonia in X-ray scans across a number of hospitals with striking accuracy was determined to be basing its predictions not on any lesions or otherwise pathologically relevant features in the images, but rather on a hospital-specific metal token that was on every image, which it used in conjunction with learning the prevalence rate of pneumonia for the hospitals from which the data was collected. As a result, when deployed on data from hospitals that it had not seen during training, the system failed to generalize \cite{pneumonia}. 
		
	In another study, it was shown that a classifier intended to detect diabetic retinopathy exhibited significant variability in performance depending on the type of camera used. The same study also showed that the same type of performance variability could be found when detecting skin-conditions across demographics with differing skin tones. \cite{damour2020underspecification}. 
	 
	\subsection{Generalization failure in other domains}
	Naturally, non-medical domains are in no way immune to generalization failure. In fact, one could easily argue that the vast majority of deep-learning pipelines fail to generalize altogether, and instead merely infer some set of inductive biases that, although perhaps causally incorrect, perform sufficiently well for general use. It has for instance been shown that CNNs trained on imagenet, one of the largest and most diverse datasets in the domain of computer vision, are heavily biased towards textural features\cite{texturebias}. Naturally, this is not necessarily causally accurate; a cat is not a cat because it has cat-like fur; nor is an elephant an elephant only because it has skin of an elephant. By manually increasing shape bias, it has been shown that the performance of such CNNs improves both in robustness to perturbations and iid accuracy.

	\begin{figure}[H]
		\includegraphics[width=\linewidth]{example-image-a}
		\caption{}
		\label{cat_elephant}
	\end{figure}
	
	Another characteristic of deep learning that supports this argument is the effectiveness of adversarial attacks \cite{adversarial_bugs_features}, which specifically target weaknesses in the inductive biases within DNNs through any number of means in an attempt to induce high rates of incorrect, yet highly confident predictions. Gradient-based adversarial attacks, for instance, use the gradients of the model to break even the most sophisticated and well-trained pipelines merely by adding some carefully crafted, yet visually imperceptible noise to the inputs \cite{adversarial_attacks}. Even without access to the gradients, there exists a multitude of so-called black-box attacks that only use output samples to generate similarly effective attacks (cite). Finally, it has been shown that adding minor visual distractions to objects, for example adding bits of tape or graffiti to stop signs, dramatically increases misclassification rates \cite{physical_attacks}. 
	
	Even benign, but nonetheless confounding perturbations also have the potential to induce failure. It has for instance been shown that sophisticated natural language processing models can and readily do fail if one adds peripheral information to the input. (Example, citation)	 
	
\section{Generalisability Theory}
	Exactly why and how DNNs seem to so persistently fail to generalize is a topic of ongoing research, and the available literature seems to suggest that the problem is multifaceted. This section is an attempt to summarize and distill the findings and analysis performed in the field. It will cover the theoretical basis of generalization and why one might expect DNNs to generalize, discuss the key characteristics of generalization failure and their origins, and finally introduce a probabilistic perspective of generalization.
	\subsection{Generalization through Empirical Risk Minimization} 
		Naturally, deep learning would not have experienced as much of a revolution in the last decade or so if there was not some semblance of an expectation that their striking performance was generalisable and performant also outside the idealized settings typically involved in research. The theoretical basis that informs this belief in (most) modern deep learning pipelines is the idea of so-called empirical risk minimization, wherein it is assumed that the dataset upon which the model is trained is a representative sample of the distribution of all possible samples in the relevant domain. In other words, it assumes that the dataset is independently and identically distributed (iid) to the domain distribution. To better understand this assumption, it is beneficial to consider the it from first principles: 
		
		At the most fundamental level, the goal of machine learning is to learn a mapping between two spaces of objects \(X\) and \(Y\). This mapping, namely the function \(f: X \rightarrow Y\), maps some input object \(x \in X\), an image for example, to a corresponding and application-relevant output object \(y \in Y\), for instance a segmentation mask or a class probabilities. It is worth noting, however, that \(f\) is not as much a function in the mathematical sense as much as it is an abstraction of whatever ground-truth relationship that the deep learning system is intended to capture, and consequently cannot typically be modelled explicitly. Instead, machine learning systems aim to find a representation of this mapping automatically by leveraging a training set \(\{x_i, y_i\}_{0...n}\) to find a sufficiently performant approximation of \(f\). This is referred to as supervised learning, and the resulting approximation found using the training set is denoted by \(h: X \rightarrow \hat{Y}\), and typically referred to as a hypothesis.  
		        
		To find such an approximation, we assume that there exists a joint probability distribution over \(X\) and \(Y\), namely \(P(x,y)\), and that the training data \(\{x_i, y_i\}_{0...n}\) is drawn from this probability distribution such that the resulting sample distribution is independent and identically distributed to \(P(x,y)\). This is the so-called iid assumption. By modelling the mapping as a joint probability distribution, one can model uncertainty in the predictions by expressing the output as a conditional probability \(P(y|x)\). In conjunction with a loss-function \(L(h(x),y)\) which measures the discrepancy between the hypothesis and the ground truth, these assumptions allows us to quantify the expected performance of a given hypothesis:
		\begin{equation}
		    R(h) = \boldsymbol{E}[L(h(x),y)] = \int L(h(x),y) dP(x,y)
		\end{equation}
		Using this framework, one can then find an iid-optimal hypothesis, often called a predictor, by finding the predictor \(h^*\) among a fixed class of functions (defined by network architecture) \(\mathcal{H}\) that minimizes risk:
		\begin{equation}
		h^* = \argmin_{h \in \mathcal{H}}R(h)
		\end{equation}
		
		Since \(P(x,y\)) is not known, however, one cannot compute \(R(h)\) explicitly. Instead, the expected risk has to be estimated empirically, i.e by finding the arithmetic average of the risk associated with each prediction by the hypothesis over the training set:
		\begin{equation}
		R_{emp}(h) = \frac{1}{n}\sum_{i=1}^{n}L(h(x_i), y_i)
		\end{equation}
		This risk can in turn be minimized with respect to the hypothesis class. This is called empirical risk minimization (ERM):
		\begin{equation}
		\hat{h} = \argmin_{h \in \mathcal{H}}R_{emp}(h)
		\end{equation}
		To reiterate, the central idea with this approach to machine learning is that the training data can be considered a finite iid sampling of the underlying distribution. As such, by the central limit theorem, the hold-out performance of the computed hypothesis will approach iid-optimal performance given a sufficient amount of training data and some sufficiently capable and regularized training procedure. This should in theory allow deep learning systems to be able to generalize, since the empirical risk in theory can approximate the true risk arbitrarily well given sufficient training data support.
	\subsection{A taxonomy of Generalization Failure Modes}
		As the analysis in \ref{case_studies} shows, ERM nonetheless readily fails to generate generalisable predictors with respect to out-of-distribution data). Understanding exactly why this is the case is a subject of ongoing study, and the literature around the matter is highly fragmented. In broad strokes, the literature attributes generalisation failure to one of the following properties:
		% Restructure
		\begin{itemize}
			\item Structural misalignment
			\item Underspecification
			\item ...
		\end{itemize}
		
		To fully understand the nuances that distinguish the respective arguments First, consider the assumptions upon which ERM is based, namely that:
		\begin{enumerate}
			\item \(f\) exists in \(\mathcal{H}\) \label{underfit}
			\item (overfitting) \label{overfit}
			\item \(\{x_i, y_i\}\) is an IID sampling of \(P(x,y)\). This is the aforementioned iid assumption. \label{structural_misalignment}
			\item \(\hat{h}\) is unique in \(\mathcal{H}\)\label{underspecification}
			\item The optimizer consistently finds \(\hat{h}\) (given that it exists and is unique)\label{opt}
		\end{enumerate}  	
		
		The behavior that violations of assumptions \ref{underfit} and \ref{overfit} is well understood and fairly easy to detect, corresponding to underfitting and overfitting respectively, but violations of the remaining assumptions result in more subtle forms of generalization failure. 
		
		The general consensus is that generalization
		failure can in broad strokes be attributed to either underspecification or structural misalignment. The following sections will attempt to summarize and synthesize the analyses within the literature, and connect each of the generalization
		failure modes they identify to the above violations.
		\subsection{Underfitting, Overfitting and Regularization}
		\subsection{Structural Misalignment and Distributional Shift}
		Generalization failure is often attributed to structural misalignment between the training domain and the deployment domain \cite{adversarial_bugs_features,shortcut_learning,IRM}. This structural misalignment can range in magnitude, from entirely differing modalities to distributional shifts that are practically imperceptible to humans, but so long as there exists such a misalignment, the models will likely experience generalization failure since there is no way of guaranteeing that the patterns that deep learning models detect are invariant to whatever distributional shift that constitutes the misalignment. 
		
		To illustrate, consider the rather pertinent example of training a model exclusively on either white-light or narrow-band endoscopy. Assume that there are two datasets, each containing samples depicting identical scenes, with the only difference being that dataset A employs white-light endoscopy, whereas dataset B employs narrow-band endoscopy. Ideally, a model trained on either dataset should generate predictors that can generalize to the other. Though one might (given a fair dose of optimism) expect the models to learn features that are invariant to lighting conditions and hence generalise well to the other dataset, this is in no way guaranteed. The causal mechanisms behind the decisions - i.e what makes a polyp a polyp - are never considered at any point in the training process. Instead, the models will simply try to leverage whatever predictive patterns they may detect in the training data, regardless of whether or not they make sense from a human perspective. The model trained on narrow-band images may for instance principally consider the textural characteristic of the polyps, which narrow-band endoscopy enhances. Conversely, the model trained on white-light, lacking access to these textural characteristics, may instead consider more color- or shape-focused properties. Naturally, if the texture-biased model is deployed in white-light endoscopy, it is not likely to succeed since its principal discriminative features no longer are particularly predictive. The color-biased model would fail when deployed in narrow-band endoscopy for the same reason. 

		\begin{figure}[H]
			\includegraphics[width=\linewidth]{example-image-b}
			\caption{}
			\label{imaging_modalities}
		\end{figure}

		Of course, though the features each model learns are not particularly representative of the broader context of what makes a polyp a polyp, they make sense when considered from the perspective of either of the two hypothetical datasets. When considering only narrow-band imaging, it makes some sense to heavily weigh the texture of the polyps. When considering only white-light imaging, it makes some sense to heavily weight the shape and colour of the polyps. This example, though illustrative, is nonetheless a bit misleading for this reason. As it turns out, DNNs are unlikely to learn these causally viable features in the first place. If such interpretable distributional shifts were the principal cause of generalization failure, generalizability could be induced by explicitly modelling the effects such shifts induce and taking this into account in the pipeline. In the aforementioned example, one could for instance train some model to map from one lighting environment to the other. Assuming a perfect model, this would imbue the model with an inherent invariance to the choice of lighting, but it is nonetheless not given that the resulting model will be perfectly generalizable.

		Though these types of distributional shifts also hold some importance when designing generalizable models. a much more pervasive and substantially more significant issue is the fact that distributional shifts are not necessarily impactful or, as will be discussed later, even perceptible to a human observer. A human would for instance not be significantly affected by noisy images, blurry images, rotated images, compressed images, and so on, whereas DNNs have been shown to be highly sensitive to these and several other forms of minor perturbations \cite{noise_robustness, corruption_robustness,adversarial_training}. 

		
		\subsubsection{Spurious correlations, shortcut learning, and Adversarial Attacks}
		This weakness to distributional shifts and structural misalignment can be attributed to the fact that DNNs do not leverage any form of causal logic to inform their decisions. Instead, they readily exploit any sufficiently predictive pattern they may observe in the data, even if they are causally nonsensical. It is for instance not necessarily the case that the model trained on the white-light dataset would be biased towards shape and size and the model trained on the narrow-band dataset would be biased towards texture. They could learn to exploit any number of arbitrary patterns so long as they are predictive and thus risk-minimizing for the given distribution. Naturally, these spurious correlations do not necessarily generalize well. To give a concrete example, consider the problem of classifying images of cows and camels as the respective animals, wherein the dataset consists of cows, pictured in grass fields and pastures, and camels, pictured in deserts. To be generous, let us assume that we have sufficient quantities of data to ensure that the pipeline is perfectly invariant to the pose of the respective animals, to lighting conditions, geometric transforms, weather, etc. One may then expect that the pipeline correctly learns to classify the two, and attains high accuracies, and indeed when evaluated on iid data - i.e cows in fields and camels in deserts, this would be entirely correct. However, what would then happen if one such predictor encountered a cow in the desert and a camel in a grass pasture? This constitutes a distributional shift, and as such we cannot expect the models to generalize. Naturally, the predictor may have learned just fine exactly what constitutes a cow and a camel, but it might just as easily learned to associate deserts with camels and pastures with cows. And from a purely statistical perspective, both are equally correct interpretations. From a causal perspective, however, it is of course entirely nonsensical to assume the respective animals are wholly defined by their surroundings.

		Of course, this argument is not limited to the domain of barnyards. DNNs may readily identify and leverage any spurious pattern in any dataset as long as they are predictive. Indeed, this phenomenon has been shown to be quite pervasive across all manner of domains, from natural language processing and computer vision to reinforcement learning and algorithmic decision making \cite{shortcut_learning}. This is referred to by a myriad of different terms, namely shortcut learning \cite{shortcut_learning}, the Clever Hans effect \cite{clever_hans}, (...)
		%Clever hans effect

		%Connection to shortcut learning
		This notion and the brittle features that result therefrom have also been identified as one of the key properties that explains the effectiveness and pervasiveness of adversarial attacks \cite{adversarial_bugs_features}. 

		Fundamentally, this all arises from (analysis wrt ERM here)		
		
		\subsection{Underspecification}
			The idea of underspecification has also been identified as a key reason for generalization failure \cite{damour2020underspecification}. A machine learning pipeline can be considered underspecificed when it can return any number of risk-equivalent predictors when evaluated on an iid holdout set, dependent only on the random variables used within the training procedure - i.e dropout, seed initialization, and so on.
			
			
		\subsection{Causality in Machine Learning}
		\subsection{A Bayesian perspective of generalization}
		
\section{Inducing robust features}
	\subsection{Adversarial Training}
		\subsection{Adversarial attacks and defences}
	\subsection{Improved Risk estimation}
		\subsubsection{Data augmentation}
		\subsubsection{Distributional modelling}
	\subsection{Bayesian Learning}
	\subsection{}

\section{Related work on Generalizable Polyp Segmentation}
	The EndoCV2021 challenge focused primarily on addressing methods to increase the robustness and generalizability. The approaches utilized by the submissions can by and large be assigned one of the following categories:
	\begin{itemize}
		\item Generalisation through regularization
		\item Generalisation through ensembles
		\item Generalisation through feature strengthening 
	\end{itemize}
		
	\section{Putting it all together}