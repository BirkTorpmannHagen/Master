\chapter{Methods and Implementation}
\setcounter{chapter}{3}
\section{\alg}
	Summarizing the key points made in chapter \ref{background}, generalization is in large part a function of the causal robustness of the features it learns. The problem then boils down to the following questions:
	\begin{enumerate}
		\item How can behaviour consistent with the causal structure of a problem be rewarded? \label{loss}
		\item How can the pertinent inductive biases that underpin this causal structure be expressed? \label{mnv}
		\item How can we optimize for causally consistent inductive biases? \label{training}
	\end{enumerate}
	
	(Justifications here)

	\subsection{Consistency Loss}
	Consistency can be expressed as the degree to which a certain model resists change when subjected to some form of perturbation. In the context of segmentation, this corresponds to the model outputting corresponding segmentation masks for both the perturbed and unperturbed images. Of course, since the perturbation is not precluded from modifying the polyps themselves through distortions and so on, it is necessary to also take into account the expected change due to the perturbation, and discount this from the overall expression. In simpler terms, the loss needs to describe the discrepancy between the expected change in the segmentation predictions due to the perturbation(s) and the actual change in the segmentation predictions. This loss will be referred to as the naked consistency loss (for reasons that will soon be made clear). Formally, this can be expressed as follows:

	Let \(Y:=\{y,\hat{y}:=f(x)\}\) be the set consisting of the segmentation predictions and masks for the unperturbed samples, and \(\epsilon(\cdot)\) be some perturbation function. Then, let \(A:=\{a:=\epsilon(y),\hat{a}:=f(\epsilon(x))\}\) be the set consisting of segmentation predictions and masks for the perturbed samples. The naked consistency loss can then be expressed as follows: 
	\begin{equation}
		L_c = \frac{1}{\sum\{y \cup a \}} \sum \{y\ominus\hat{y}\ominus a\ominus\hat{a}\}
	\end{equation}
	Where \(\ominus \) denotes the symmetric difference. 
	
	The attentive reader may have noticed that this loss is zero not only if the predictions are both correct and consistent with one another, but also if the predictions are both incorrect, as long as they are consistent with one another. The reasoning behind this is that consistent behavior should be rewarded even if the model has not quite learned how to perform to an adequate standard. To illustrate, consider once more the example from chapter \ref{background} with the narrow-band and white-light polyp-segmentation datasets. Assume that the perturbation function simply maps between the respective lighting modalities. In this case, the loss will reward the model if it predicts identical segmentations regardless of lighting conditions, even if they are both mostly incorrect. It will nevertheless be trying to infer patterns which are invariant to lighting, however, and consequently be causally stronger than a pipeline wherein the model is permitted to be inconsistent depending on lightning conditions. 
	
	Of course, using this loss in isolation is not really practical. For one, the model will have no way of knowing what the actual intent behind it is, and moreover the model will most likely learn the simplest possible interpretation of consistency and simply predict the same segmentation every time, regardless of the lighting conditions and for that matter regardless of the input. These are some pretty broad local minima, and there would naturally be a significant number of risk-equivalent predictors, which of course in accordance with the analysis in \ref{background} constitutes generalisation failure on its own. Thus, it has to be combined with a task-specific loss, which for the polyp-segmentation task could be Dice-loss, Jaccard-Loss, binary cross entropy, etc. 
	
	Naturally, jointly optimizing for these two often conflicting objectives - overall task performance vs consistency - is not as straight forward as it may seem. The naive approach would be to simply add the task-loss and the naked consistency:
	\begin{equation}
		L = L_{task} + L_c
	\end{equation}
	This, however, leads to unstable behaviour and often inhibits convergence, since the consistency term quickly starts gaining precedence if the segmentation task is difficult (see appendix). Consequently, adaptive weighing is required. To avoid getting stuck in broad local minima early, the training should in the early stages be biased towards acheiving semi-decent segmentation performance. Later on, when segmentation performance is starting to become reasonably high, the pipeline should shift towards trying to optimize for consistency. This can be acheived by weighing each term according to a desired performance metric, for instance intersection over union (IoU):
	\begin{equation}
		L = (1-IoU)\times L_{task} + IoU \times L_c
	\end{equation}
	If Jaccard loss is used, this is also equivalent to:
	\begin{equation}
		L = {L_{jac}}^2 + (1-L_{jac})\times L_c
	\end{equation}
	
  	\subsection{Model of natural variation}
  		In order to account for any natural variation one may expect to find in deployment, it is necessary to construct a model which can parameterize the variability that is encountered. This model of natual variation, or MNV, can then be leveraged in conjunction with consistency loss to facilitate the learning of features that are robust to the types of variation the MNV defines. Naturally, there is no way of knowing the full extent of all the types of variability one may find in the wild, but it may nonetheless be sufficient to model some subset thereof. This, naturally, requires some knowledge of the domain from which the dataset is collected. Similarly to how adding rotational augmentations is a bad idea for classification of hand-written numbers, certain transformations may or may not be suitable for use within a MNV.
  		
  		In the case of polyp-segmentation, it is clear that it is necessary to account for variability in for instance lighting, image-resolution, polyp-size, polyp-shape, polyp-location, camera-quality, color-shifts, blurs, optical distortions, and affine transformations. Thus, a model is required that can (more or less) parametrize this variability. Broadly speaking, these transformations can be categorized as follows:
  		\begin{itemize}
  			\item Pixel-wise variability, which affect only the image, i.e color-shifts, brightness shifts, contrast-shifts, lighting, blurs etc
  			\item Geometric variability, which affect both the image and the segmentation mask by some parametrizable quantity, i.e affine transforms and distortions
  			\item Manifold variablity, which affects both the image and the segmentation mask depending on a learned model of the distribution,  i.e the size, shape and location of polyps
  		\end{itemize}
  		Pixel-wise variability and geometric variability can be modeled fairly trivially through the use of the same transformations typically used in conventional data-augmentation. Manifold-variability, however, is somewhat more difficult. Similar to how \cite{modelbased} and \cite{cyclegan} employ cross-dataset style-transfer to account for more implicit distributional shift, it is necessary to find some way to model the distributional properties of the data, and then apply perturbations using the resulting model. Since both the size, shape, and position of polyps can be expected to vary, a model that can change all these factors is necessary. To this end, an inpainting model can be constructed. In particular, a GAN-inpainter. 
  		\subsubsection{Gan-based polyp inpainting}
  		\subsubsection{Geometric and pixel-wise transformations}
  	\subsection{Training methods}
		\subsubsection{Consistency Training}
		\subsection{Adversarial Consistency Training}
		\subsection{Augmentation only training}
\section{Baselines and Metrics}
    \subsection{Baseline Models}
    \subsection{Performance Metrics}
    \subsection{Datasets}
\section{Experiments}
    \subsection{MNV-testing}
	\subsection{Training methods}
