\chapter{Analysis}
\addcontentsline{toc}{chapter}{Analysis}
\setcounter{chapter}{5}

\section{Augmentation Robustness and Consistency Loss}
As the results show, the performance of the pipeline that merely used augmentations is more or less equivalent to the performance exhibited by the modified pipeline. There is a very good reason for this: Consistency loss is mathematically equivalent to data augmentation, up to the choice of hyperparemeters - i.e augmentation probability, learning rates, etc. This section presents a proof of this fact, along with a theoretical analysis of how data augmentation affects the pipeline. 
\subsection{Data augmentation}
Let \(Y:=\{y,\hat{y}:=f(x)\}\) be the set consisting of the segmentation predictions and masks for the unaugmented samples, and \(A:=\{a:=MNV(y),\hat{a}:=f(MNV(x))\}\) be the set consisting of segmentation predictions and masks for the augmented samples. Finally, let \(Z:=\{z, \hat{z}\} \in_R \{Y, A\} \). The loss function subject to data augmentation can then be expressed as \(L(Z \in_R Y,A)\), where L is any loss function. For the sake of simplicity in remaining calculations, this will be treated as the Jaccard loss, i.e \(L(y,\hat{y}):=1-\sum y\cap \hat{y} / \sum y \cup \hat{y}\) 
\begin{align*}
L(Z \in_R Y,A)  
\end{align*}
\subsection{Consistency loss}

\begin{align}
L_s &= \frac{1}{\sum \hat{y} \cup \hat{a}} \sum \hat{y}\ominus y \\
L_c &= \frac{1}{\sum \hat{y} \cup \hat{a}}\sum  [\hat{y}\ominus y\ominus \hat{a} \ominus a  ]\\
L_{c+s} &= L_c(Y, A) + L_s(Y)\\
&=\frac{1}{\sum \hat{y} \cup \hat{a}} \Bigg[\sum  \{\hat{y}\ominus y\ominus \hat{a} \ominus a  \} + \sum \{\hat{y}\ominus y\}  \Bigg]\\
\begin{split}
&=\frac{1}{\sum \hat{y} \cup \hat{a}} \Bigg[\sum \{\hat{y}\ominus y\} + \sum  \{\hat{a}\ominus a\}\\
&-\sum \{\setminus y\cap \hat{y} \cap a \cap \hat{a}\}\cup\{y\cap \setminus \hat{y} \cap a \cap \hat{a}\}\cup \\ 
&\{y\cap \hat{y} \cap \setminus a \cap \hat{a}\}\cup\{y\cap \hat{y} \cap a \cap \setminus \hat{a}\}\\
&-\sum\{\setminus y\cap \hat{y} \cap \setminus a \cap \hat{a}\} \cup\{y\cap \setminus \hat{y} \cap \setminus a \cap \hat{a}\} -\\
&\cup\{\setminus y\cap \hat{y} \cap a \cap \setminus\hat{a}\}\cup\{y\cap \setminus \hat{y} \cap a \cap \setminus \hat{a}\} \\ 
&+\sum \{\hat{y}\ominus y\}   \Bigg]\\
\end{split}\\
\begin{split}
&=2L_s(y, \hat{y})+L_s(a, \hat{a}) +\frac{1}{\sum \hat{y} \cup \hat{a}} \Bigg[\\
&-\sum \{\setminus y\cap \hat{y} \cap a \cap \hat{a}\}\cup\{y\cap \setminus \hat{y} \cap a \cap \hat{a}\}\cup \\ 
&\{y\cap \hat{y} \cap \setminus a \cap \hat{a}\}\cup\{y\cap \hat{y} \cap a \cap \setminus \hat{a}\}\\
&-\sum\{\setminus y\cap \hat{y} \cap \setminus a \cap \hat{a}\} \cup\{y\cap \setminus \hat{y} \cap \setminus a \cap \hat{a}\} -\\
&\cup\{\setminus y\cap \hat{y} \cap a \cap \setminus\hat{a}\}\cup\{y\cap \setminus \hat{y} \cap a \cap \setminus \hat{a}\}   \Bigg]
\end{split} \label{loss_proof}
\end{align}
The non-loss terms in equation \ref{loss_proof} are proper subsets of the symmetric difference of the mask and segmentation across either dataset. The component of the loss that corresponds to these terms consequently grows in proportion to both \(L_s(y, \hat{y})\) and \(L_s(a, \hat{a})\). \(L_{c+s}\) and \(L_{sy+sa}\) are therefore monotonically correlated - i.e, when one grows, the other grows with it, and when one falls, the other one falls with it. 

\subsection{Adversarial Dice}
\begin{align*}
L=\frac{1}{2} L(a, \hat{a}) +  \frac{1}{2} L(y, \hat{y})
\end{align*}
This should be asymptotically equivalent to data augmentation wit p=0.5