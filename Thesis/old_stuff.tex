	The term "generalizability" is used widely in literature, despite rarely being particularly well defined. Often, generalizability refers merely to the state of a predictor as "not overfitted". In other words, that it maintains sufficient performance across the training, validation, and test-sets. This, however, typically neglects the more salient aspects of the performance of the pipeline; namely how it behaves when deployed in practical settings, on data that may be distributed differently from the training dataset. Consider for instance the problem of detecting and classifying traffic signs. Though it is relatively trivial to achieve decent performance on such a task when training and evaluating on one specific dataset, it is another matter entirely to make sure the resulting predictors are robust to any and all forms of variability one might expect to see when deployed in a practical setting. If the training data was for instance collected from a region with a dry, temperate climate, it might not come as a surprise that it will not perform as well when deployed in an area prone to snowfall, fog, or generally low visibility. Of course, this could be mitigated by ensuring that the training data contains samples from a wide variety of climates, but this really only affects the robustness of the pipeline to differing climates. It does not necessarily ensure that the resulting predictors learn to ignore weather effects entirely, and as such it may nonetheless fail if it encounters something it has not been explicitly trained on. This is made especially evident in the study of black-box adversarial attacks: %examples ...
	        
	In other words, merely being robust to a limited class of perturbations or variability is not sufficient to deem a pipeline as generalizable. The pipeline must not merely learn to be right, but right for the right reasons. If this is achieved, robustness to distributional shifts follows. The system outlined above should in other words not only be robust to snow or rain or fog, but to be able to ignore the effects thereof entirely. A perfectly generalizable pipeline shouuld return a weather-invariant predictor every time, and for that matter maintain invariance to any and all non-destructive distributional shifts. 
	        
	The term generalizability will as such in this thesis refer to the ability to infer the right inductive biases from an incomplete dataset. This is as opposed to robustness, which denotes the ability of a pipeline to maintain its performance across certain distributional shifts. Generalizability is as a consequence not as much of a measurable quantity as much as it is an emergent property of a well-designed pipeline. 
	        
	This section will explore the concept of generalizability in further detail. It will outline how typical deep learning-based systems aim to achieve generalizability, why it nonetheless often fails to do so, and how one can analyse such generalizability failure.