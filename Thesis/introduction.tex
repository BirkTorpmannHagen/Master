    \chapter{Introduction}
    \addcontentsline{toc}{chapter}{Introduction}
    \setcounter{chapter}{1}
    %research goals
    %research motivation
    %thesis outline
    Colorectal cancer is one of the leading causes of cancer related deaths, causing approximately 900 thousand deaths worldwide per year (cite). Early detection thereof is as a consequence of significant importance. Polyps are often an early warning-sign of developing tumour, and early detection thereof can as a result significantly reduce fatality rates. Polyps are, however, often missed during colonoscopies, owing to the significant variability in the shapes and sizes of polyps, as well as the high degrees of similarity to surrounding tissue. Automatic segmentation of polyps via deep learning has the potential to significantly increase the likelihood of early detection and thus effective treatment. 
    
    Though there has been a wealth of work dedicated to developing such systems, with promising results, recent work in the field has highlighted that deep neural networks (DNNs) readily fail to maintain performance when deployed outside of lab-conditions. This is known as generalization failure, and has been shown to be ubiquitous across practically every application of deep learning. The deep learning community is still in the early stages of understanding exactly how and why such generalization failure is so ubiquitous. Consqeuently, developing methods and frameworks to combat generalisation remains an open problem. 

    This thesis attempts to address this problem by synthesizing and systematizing recent work in generalizability, generalizable methods, and recent attempts at inducing generalizability in polyp segmentation as presented in the EndoCV2021 challenge. A novel approach to increasing generalizability, based on recent work in the field, is also presented. The approach, named \alg,  works by employing specific augmentation strategies to produce a so-called model of natural variation, intended to encapsulate the variability one might expect across different datasets and hence assist in inducing invariances in the model. This is acheived through the use of a specifically tailored loss, referred to as consistency loss, which punishes inconsistent predictions across the augmented and unaugmented folds irrespective of the correctness of the predictions. This endows the pipeline with the ability to more readily infer causally viable inductive biases by explicitly forcing the model to be robust to any combination of the aforementioned transformations. 
    
    Generalizability is then measured by evaluating several vanilla-pipelines consisting of several models on a number of separate datasets, which itsthen compared toroot causes, the results of the modified piresults show that (...)
    
   
%      What is the use of a Nifty Gadget?
        %Segmentation of polyps; importance
%      What is the problem?
        % Generalizability failure
%      How can it be solved?
        % Little research
%      What are the previous approaches?
        % ???
%      What is your approach?

%      Why do it this way?
%      What are your results?
%      Why is this better?
%      Is this a new approach?
%      Why haven't anyone done it before?